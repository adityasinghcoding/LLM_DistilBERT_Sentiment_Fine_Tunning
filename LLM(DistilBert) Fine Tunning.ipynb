{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing libraries: For transformers & model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate\n",
    "# pip install transformers datasets accelerate torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM (DistilBERT) Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine Tunning DistilBERT...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:01:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679681</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.656687</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.591665</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.490888</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457760</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.394464</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.369610</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.387056</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.399095</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.382370</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('H:/Resume Projects/LLM Fine Tunning/Tokenizer\\\\tokenizer_config.json',\n",
       " 'H:/Resume Projects/LLM Fine Tunning/Tokenizer\\\\special_tokens_map.json',\n",
       " 'H:/Resume Projects/LLM Fine Tunning/Tokenizer\\\\vocab.txt',\n",
       " 'H:/Resume Projects/LLM Fine Tunning/Tokenizer\\\\added_tokens.json',\n",
       " 'H:/Resume Projects/LLM Fine Tunning/Tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate # For loading evaluation metrics\n",
    "from datasets import load_dataset # For loading datasets\n",
    "from transformers import(\n",
    "   AutoTokenizer, # For text tokenization\n",
    "   AutoModelForSequenceClassification, # Classification model\n",
    "   TrainingArguments, # Training configuration\n",
    "   Trainer # For Handling training loop\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# ------------------------\n",
    "# Load IMDB movie review dataset (positive & negative sentiment labels)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Initialize tokenizer for DistilBERT (smaller & faster version of BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define function to process text\n",
    "def tokenize_function(examples):\n",
    "   \"\"\"\n",
    "    Converts raw text into model-readable tokens\n",
    "    - padding=\"max_length\": Fills shorter texts with zeros\n",
    "    - truncation=True: Cuts texts longer than max_length\n",
    "    - max_length=128: Use shorter sequences for CPU efficiency\n",
    "    \"\"\"\n",
    "   return tokenizer(examples[\"text\"],\n",
    "   padding=\"max_length\",\n",
    "   truncation=True,\n",
    "   max_length=128\n",
    ")\n",
    "\n",
    "# Apply tokenization to entire dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched= True)\n",
    "\n",
    "# Create smaller datasets for faster training on CPU\n",
    "# We use tiny subsets for demonstration purposes\n",
    "train_dataset = tokenized_dataset['train'].shuffle(seed=42).select(range(500)) # 500 training examples\n",
    "eval_dataset = tokenized_dataset['test'].shuffle(seed=42).select(range(100)) # 100 evaluation examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. SETUP MODEL\n",
    "# --------------\n",
    "# Load pre-trained DistilBERT with classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   \"distilbert-base-uncased\", # Smaller/faster model\n",
    "   num_labels=2 # Two classes: positive/negative\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 3. CONFIGURE TRAINING\n",
    "# ---------------------\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"H:/Resume Projects/LLM Fine Tunning/Outputs\", # Where to save outputs   \n",
    "   evaluation_strategy=\"steps\", # Evaluate every X steps\n",
    "   eval_steps=50, # Run evaluation every 50 steps\n",
    "   learning_rate=1e-5, # How quickly model updates (small for fine-tuning)\n",
    "   per_device_train_batch_size=2, # Number of examples per batch (small for CPU)\n",
    "   per_device_eval_batch_size=2,\n",
    "   num_train_epochs=2, # Full passes through the dataset\n",
    "   weight_decay=0.01, # Regularization to prevent overfitting\n",
    "   logging_dir=\"H:/Resume Projects/LLM Fine Tunning/Training Logs\", # Save training logs\n",
    "   save_strategy=\"no\", # Don't save checkpoints (saves space)\n",
    "   no_cuda=True, # Force CPU usage (no GPU)\n",
    "   dataloader_num_workers=2, # Use 2 CPU cores for data loading\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 4. SETUP EVALUATION\n",
    "# -------------------\n",
    "# We'll use accuracy as our evaluation metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_p):\n",
    "   \"\"\"Calculates accuracy from model predictions\"\"\"\n",
    "   logits, labels = eval_p  # Model outputs (logits) vs true labels\n",
    "   predictions = np.argmax(logits, axis=-1)  # Convert logits to predictions (0 or 1)\n",
    "   return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. INITIALIZE TRAINER\n",
    "# ---------------------\n",
    "trainer = Trainer(\n",
    "   model=model, # Our classification model\n",
    "   args=training_args, # Training configuration\n",
    "   train_dataset=train_dataset, # Training data\n",
    "   eval_dataset=eval_dataset, # Evaluation data\n",
    "   compute_metrics=compute_metrics, # How to calculate metrics\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 6. START TRAINING\n",
    "# -----------------\n",
    "print(\"Fine Tunning DistilBERT...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Saving tuned final model:\n",
    "model.save_pretrained(\"H:/Resume Projects/LLM Fine Tunning/Tuned Model\")\n",
    "tokenizer.save_pretrained(\"H:/Resume Projects/LLM Fine Tunning/Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
